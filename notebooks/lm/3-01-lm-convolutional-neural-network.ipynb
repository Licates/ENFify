{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T22:08:38.732308Z","iopub.status.busy":"2024-09-06T22:08:38.731908Z","iopub.status.idle":"2024-09-06T22:08:51.657993Z","shell.execute_reply":"2024-09-06T22:08:51.656736Z","shell.execute_reply.started":"2024-09-06T22:08:38.732271Z"},"trusted":true},"outputs":[],"source":"# %load_ext autoreload\n# %autoreload 2\n!pip install torchsummary"},{"cell_type":"markdown","metadata":{},"source":" # Imports, Constants, Classes"},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T22:08:51.660683Z","iopub.status.busy":"2024-09-06T22:08:51.660343Z","iopub.status.idle":"2024-09-06T22:08:51.713952Z","shell.execute_reply":"2024-09-06T22:08:51.712979Z","shell.execute_reply.started":"2024-09-06T22:08:51.66065Z"},"trusted":true},"outputs":[],"source":"from pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchsummary import summary\nfrom tqdm.notebook import tqdm"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"try:\n    from enfify import PROCESSED_DATA_DIR\nexcept ImportError:\n    DATA_DIR = Path(\"/kaggle/input/enf-datd-frequency-features\")\nelse:\n    DATA_DIR = PROCESSED_DATA_DIR\n\n# Ensure reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)"},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T22:08:51.715605Z","iopub.status.busy":"2024-09-06T22:08:51.715232Z","iopub.status.idle":"2024-09-06T22:08:51.756162Z","shell.execute_reply":"2024-09-06T22:08:51.75523Z","shell.execute_reply.started":"2024-09-06T22:08:51.715562Z"},"trusted":true},"outputs":[],"source":"# Use CUDA if a GPU is available\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\nprint(\"Using device\", device)"},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T22:08:51.758798Z","iopub.status.busy":"2024-09-06T22:08:51.758475Z","iopub.status.idle":"2024-09-06T22:08:51.796655Z","shell.execute_reply":"2024-09-06T22:08:51.79572Z","shell.execute_reply.started":"2024-09-06T22:08:51.758764Z"},"trusted":true},"outputs":[],"source":"# Hyperparameters\nNUM_EPOCHS = 2_000\nBATCH_SIZE = 32\nLEARNING_RATE = 0.1\nNOMINAL_ENF = 50.0\nTEST_SIZE = 0.2"},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T22:08:51.839829Z","iopub.status.busy":"2024-09-06T22:08:51.839493Z","iopub.status.idle":"2024-09-06T22:08:51.882441Z","shell.execute_reply":"2024-09-06T22:08:51.881572Z","shell.execute_reply.started":"2024-09-06T22:08:51.839796Z"},"trusted":true},"outputs":[],"source":"class OneDCNN(nn.Module):\n    def __init__(self):\n        super(OneDCNN, self).__init__()\n        \n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=75, kernel_size=130, stride=1) # Convolutional Layer\n        self.pool = nn.MaxPool1d(kernel_size=2861-129)  # Max-Pooling Layer: Adjust kernel size to pool to (75, 1)\n        \n        self.fc1 = nn.Linear(75, 128) # Dense Layer to go from (75) to (128)\n        self.fc2 = nn.Linear(128, 2) # Output Layer to go from (128) to (10) or 2 for binary classification\n        \n    def forward(self, x):\n        # Apply convolutional layer with ReLU activation\n        x = F.relu(self.conv1(x))\n        \n        # Apply max pooling\n        x = self.pool(x) # Shape after pooling: (batch_size, 75, 1)\n        \n        # Transpose to get (batch_size, 1, 75)\n        x = x.squeeze(-1)  # Squeeze to remove the last dimension -> (batch_size, 75)\n        x = x.unsqueeze(1)  # Add a new dimension to get (batch_size, 1, 75)\n        \n        # Apply fully connected layer to get (batch_size, 1, 128)\n        x = F.relu(self.fc1(x.squeeze(1)))  # Remove dimension for dense layer and apply ReLU\n        x = x.unsqueeze(1)  # Add back the dimension -> (batch_size, 1, 128)\n        \n        # Apply final output layer to get (batch_size, 1, 2)\n        x = self.fc2(x.squeeze(1))  # Remove dimension, apply final dense layer\n        \n        # Apply softmax to get class probabilities\n        x = F.softmax(x, dim=1)  # Softmax over the class dimension (dim=1)\n        \n        return x\n\nclass EarlyStopping:\n    def __init__(self, patience=10, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss > self.best_loss - self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.counter = 0"},{"cell_type":"markdown","metadata":{},"source":" # Initializing"},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T22:08:51.92622Z","iopub.status.busy":"2024-09-06T22:08:51.925522Z","iopub.status.idle":"2024-09-06T22:08:52.234239Z","shell.execute_reply":"2024-09-06T22:08:52.233224Z","shell.execute_reply.started":"2024-09-06T22:08:51.926155Z"},"trusted":true},"outputs":[],"source":"# Initialize the model\nmodel = OneDCNN().to(device)\n\n# Load numpy data\ndsets = [\"Carioca1\", \"Carioca2\", \"Synthetic\", \"WHU_ref\"]\nfiles = sorted(sum([list((DATA_DIR / dset).glob(\"*.npy\")) for dset in dsets], list()))\nlabels = [int(\"tamp\" in file.stem) for file in files]\ndata = np.stack([np.load(file) for file in files])\n\n# Clip edges\ndata = [seq[40:-40] for seq in data]\n\n# Pad sequences\ndata_padded = [torch.tensor(seq, dtype=torch.float32) for seq in data]\ndata_padded = pad_sequence(data_padded, batch_first=True, padding_value=NOMINAL_ENF)\n\n# Berechne den Median und den IQR jeder Zeitreihe\ndata_median = np.median(data_padded.numpy(), axis=1, keepdims=True)\ndata_iqr = np.percentile(data_padded.numpy(), 75, axis=1, keepdims=True) - np.percentile(data_padded.numpy(), 25, axis=1, keepdims=True)\n\n# Normalisiere die Zeitreihen\ndata_normalized = (data_padded.numpy() - data_median) / data_iqr\n\n# Konvertiere die normalisierten Daten zur√ºck zu einem Tensor\ndata_normalized = torch.tensor(data_normalized, dtype=torch.float32)\n\ninput_size = (1, data_normalized.shape[1])\n\n# Split the data into training, validation, and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data_normalized, labels, test_size=TEST_SIZE, random_state=0, stratify=labels\n)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=TEST_SIZE, random_state=0, stratify=y_train\n)\n\n# Convert data to tensors\nX_train = torch.Tensor(X_train).unsqueeze(1).to(device)\ny_train = torch.LongTensor(y_train).to(device)\nX_val = torch.Tensor(X_val).unsqueeze(1).to(device)\ny_val = torch.LongTensor(y_val).to(device)\nX_test = torch.Tensor(X_test).unsqueeze(1).to(device)\ny_test = torch.LongTensor(y_test).to(device)\n\n# # Check label distribution\n# unique, counts = np.unique(y_train.cpu(), return_counts=True)\n# class_distribution = dict(zip(unique, counts))\n# print(f\"y_train distribution: {class_distribution}\")\n# unique, counts = np.unique(y_val.cpu(), return_counts=True)\n# class_distribution = dict(zip(unique, counts))\n# print(f\"y_val distribution: {class_distribution}\")\n# unique, counts = np.unique(y_test.cpu(), return_counts=True)\n# class_distribution = dict(zip(unique, counts))\n# print(f\"y_test distribution: {class_distribution}\")\n\n# Create TensorDatasets\nX_train_dataset = TensorDataset(X_train, y_train)\nX_val_dataset = TensorDataset(X_val, y_val)\nX_test_dataset = TensorDataset(X_test, y_test)\n\n# Data loaders\nX_train_loader = DataLoader(X_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nX_val_loader = DataLoader(X_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\nX_test_loader = DataLoader(X_test_dataset, batch_size=BATCH_SIZE, shuffle=False)"},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T22:09:25.445989Z","iopub.status.busy":"2024-09-06T22:09:25.445272Z","iopub.status.idle":"2024-09-06T22:09:25.738141Z","shell.execute_reply":"2024-09-06T22:09:25.737192Z","shell.execute_reply.started":"2024-09-06T22:09:25.44595Z"},"trusted":true},"outputs":[],"source":"# Plot a sample from the data\nplt.figure(figsize=(10, 2))\nplt.plot(data_normalized[0])\nplt.title(\"Sample Data Plot\")\nplt.show()\n"},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T22:08:52.511174Z","iopub.status.busy":"2024-09-06T22:08:52.510879Z","iopub.status.idle":"2024-09-06T22:08:52.553382Z","shell.execute_reply":"2024-09-06T22:08:52.552405Z","shell.execute_reply.started":"2024-09-06T22:08:52.511141Z"},"trusted":true},"outputs":[],"source":"# Print a single batch from the DataLoader\ninputs, labels = next(iter(X_train_loader))\nprint(f\"Input batch shape: {inputs.shape}\")\nprint(f\"Labels batch: {labels}\")\n"},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T22:08:52.554949Z","iopub.status.busy":"2024-09-06T22:08:52.554573Z","iopub.status.idle":"2024-09-06T22:08:52.594981Z","shell.execute_reply":"2024-09-06T22:08:52.594135Z","shell.execute_reply.started":"2024-09-06T22:08:52.554906Z"},"trusted":true},"outputs":[],"source":"# Initialize loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T22:08:52.596462Z","iopub.status.busy":"2024-09-06T22:08:52.596141Z","iopub.status.idle":"2024-09-06T22:08:52.636944Z","shell.execute_reply":"2024-09-06T22:08:52.63605Z","shell.execute_reply.started":"2024-09-06T22:08:52.59643Z"},"trusted":true},"outputs":[],"source":"# Display model summary\nsummary(model, input_size=input_size)"},{"cell_type":"markdown","metadata":{},"source":"# Training"},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T22:08:52.638476Z","iopub.status.busy":"2024-09-06T22:08:52.638077Z","iopub.status.idle":"2024-09-06T22:09:06.157039Z","shell.execute_reply":"2024-09-06T22:09:06.156221Z","shell.execute_reply.started":"2024-09-06T22:08:52.638424Z"},"trusted":true},"outputs":[],"source":"# Initialize EarlyStopping object\nearly_stopping = EarlyStopping(patience=10, min_delta=0.001)\n\n# Training and validation loop\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nfor epoch in tqdm(range(NUM_EPOCHS), desc=\"Training Progress\"):\n    # Training\n    model.train()\n    train_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    for inputs, labels in X_train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n        _, predicted = torch.max(outputs, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n\n    train_loss /= len(X_train_loader)\n    train_losses.append(train_loss)\n    train_accuracy = correct_train / total_train\n    train_accuracies.append(train_accuracy)\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct_val = 0\n    total_val = 0\n    with torch.no_grad():\n        for inputs, labels in X_val_loader:\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total_val += labels.size(0)\n            correct_val += (predicted == labels).sum().item()\n\n    val_loss /= len(X_val_loader)\n    val_losses.append(val_loss)\n    val_accuracy = correct_val / total_val\n    val_accuracies.append(val_accuracy)\n\n    if (epoch + 1) % 100 == 0 or epoch == 0:\n        tqdm.write(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n\n    # Check early stopping condition\n    early_stopping(val_loss)\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break"},{"cell_type":"markdown","metadata":{},"source":"# Testing"},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T22:09:06.158635Z","iopub.status.busy":"2024-09-06T22:09:06.158329Z","iopub.status.idle":"2024-09-06T22:09:06.233066Z","shell.execute_reply":"2024-09-06T22:09:06.23222Z","shell.execute_reply.started":"2024-09-06T22:09:06.158603Z"},"trusted":true},"outputs":[],"source":"# Testing phase\nmodel.eval()\ntest_loss = 0.0\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for inputs, labels in X_test_loader:\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\ntest_loss /= len(X_test_loader)\ntest_accuracy = correct / total\n\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T22:09:06.23437Z","iopub.status.busy":"2024-09-06T22:09:06.234057Z","iopub.status.idle":"2024-09-06T22:09:06.773091Z","shell.execute_reply":"2024-09-06T22:09:06.772139Z","shell.execute_reply.started":"2024-09-06T22:09:06.234338Z"},"trusted":true},"outputs":[],"source":"# Plot the training and validation losses and accuracies\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label=\"Train Loss\", alpha=.5)\nplt.plot(val_losses, label=\"Validation Loss\", alpha=.5)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Training and Validation Loss\")\n\nplt.subplot(1, 2, 2)\nplt.plot(train_accuracies, label=\"Train Accuracy\", alpha=.5)\nplt.plot(val_accuracies, label=\"Validation Accuracy\", alpha=.5)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.title(\"Training and Validation Accuracy\")\n\nplt.show()\n\n# Save the model\ntorch.save(model.state_dict(), \"model.pth\")\nprint(\"Model saved successfully!\")"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5748547,"sourceId":9457391,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}