{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:40:03.828609Z","iopub.status.busy":"2024-09-30T13:40:03.827952Z","iopub.status.idle":"2024-09-30T13:40:18.129772Z","shell.execute_reply":"2024-09-30T13:40:18.12861Z","shell.execute_reply.started":"2024-09-30T13:40:03.828568Z"},"trusted":true},"outputs":[],"source":["# %load_ext autoreload\n","# %autoreload 2\n","!pip install torchsummary"]},{"cell_type":"markdown","metadata":{},"source":[" # Imports, Constants, Classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:40:18.132603Z","iopub.status.busy":"2024-09-30T13:40:18.132192Z","iopub.status.idle":"2024-09-30T13:40:18.143786Z","shell.execute_reply":"2024-09-30T13:40:18.142787Z","shell.execute_reply.started":"2024-09-30T13:40:18.132559Z"},"trusted":true},"outputs":[],"source":["from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import RobustScaler\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchsummary import summary\n","from tqdm.notebook import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:40:18.14537Z","iopub.status.busy":"2024-09-30T13:40:18.14505Z","iopub.status.idle":"2024-09-30T13:40:18.159763Z","shell.execute_reply":"2024-09-30T13:40:18.158632Z","shell.execute_reply.started":"2024-09-30T13:40:18.145337Z"},"trusted":true},"outputs":[],"source":["try:\n","    from enfify import PROCESSED_DATA_DIR\n","except ImportError:\n","    DATA_DIR = Path(\"/kaggle/input/enf-datd-frequency-features\")\n","else:\n","    DATA_DIR = PROCESSED_DATA_DIR\n","\n","# Ensure reproducibility\n","torch.manual_seed(0)\n","np.random.seed(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:40:18.162621Z","iopub.status.busy":"2024-09-30T13:40:18.162188Z","iopub.status.idle":"2024-09-30T13:40:18.168364Z","shell.execute_reply":"2024-09-30T13:40:18.167435Z","shell.execute_reply.started":"2024-09-30T13:40:18.162576Z"},"trusted":true},"outputs":[],"source":["# Use CUDA if a GPU is available\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","print(\"Using device\", device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:40:18.170898Z","iopub.status.busy":"2024-09-30T13:40:18.169702Z","iopub.status.idle":"2024-09-30T13:40:18.178495Z","shell.execute_reply":"2024-09-30T13:40:18.177559Z","shell.execute_reply.started":"2024-09-30T13:40:18.170852Z"},"trusted":true},"outputs":[],"source":["# Hyperparameters\n","NUM_EPOCHS = 300\n","BATCH_SIZE = 32\n","LEARNING_RATE = 0.002\n","NOMINAL_ENF = 50.0\n","TEST_SIZE = 0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:40:18.180266Z","iopub.status.busy":"2024-09-30T13:40:18.179789Z","iopub.status.idle":"2024-09-30T13:40:18.210403Z","shell.execute_reply":"2024-09-30T13:40:18.209638Z","shell.execute_reply.started":"2024-09-30T13:40:18.180223Z"},"trusted":true},"outputs":[],"source":["class OneDCNN(nn.Module):\n","    def __init__(self):\n","        super(OneDCNN, self).__init__()\n","        \n","        self.conv1 = nn.Conv1d(in_channels=1, out_channels=75, kernel_size=130, stride=1) # Convolutional Layer\n","        self.pool = nn.MaxPool1d(kernel_size=491-80-130+1)  # Max-Pooling Layer: Adjust kernel size to pool to (75, 1)\n","        \n","        self.fc1 = nn.Linear(75, 128) # Dense Layer to go from (75) to (128)\n","        self.fc2 = nn.Linear(128, 2) # Output Layer to go from (128) to (10) or 2 for binary classification\n","        \n","    def forward(self, x):\n","        # Apply convolutional layer with ReLU activation\n","        x = F.relu(self.conv1(x))\n","        \n","        # Apply max pooling\n","        x = self.pool(x) # Shape after pooling: (batch_size, 75, 1)\n","        \n","        # Transpose to get (batch_size, 1, 75)\n","        x = x.squeeze(-1)  # Squeeze to remove the last dimension -> (batch_size, 75)\n","        x = x.unsqueeze(1)  # Add a new dimension to get (batch_size, 1, 75)\n","        \n","        # Apply fully connected layer to get (batch_size, 1, 128)\n","        x = F.relu(self.fc1(x.squeeze(1)))  # Remove dimension for dense layer and apply ReLU\n","        x = x.unsqueeze(1)  # Add back the dimension -> (batch_size, 1, 128)\n","        \n","        # Apply final output layer to get (batch_size, 1, 2)\n","        x = self.fc2(x.squeeze(1))  # Remove dimension, apply final dense layer\n","        \n","        # Apply softmax to get class probabilities\n","        x = F.softmax(x, dim=1)  # Softmax over the class dimension (dim=1)\n","        \n","        return x\n","\n","class EarlyStopping:\n","    def __init__(self, patience=10, min_delta=0):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.best_loss = None\n","        self.early_stop = False\n","\n","    def __call__(self, val_loss):\n","        if self.best_loss is None:\n","            self.best_loss = val_loss\n","        elif val_loss > self.best_loss - self.min_delta:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_loss = val_loss\n","            self.counter = 0\n","\n","# Initialize the model\n","model = OneDCNN().to(device)"]},{"cell_type":"markdown","metadata":{},"source":[" # Initializing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:40:18.211899Z","iopub.status.busy":"2024-09-30T13:40:18.211583Z","iopub.status.idle":"2024-09-30T13:41:47.341099Z","shell.execute_reply":"2024-09-30T13:41:47.340053Z","shell.execute_reply.started":"2024-09-30T13:40:18.211865Z"},"trusted":true},"outputs":[],"source":["# Load numpy data\n","dsets = [\"Carioca1\", \"Carioca2\", \"Synthetic\", \"WHU_ref\"]\n","# dsets = [\"Carioca1\", \"Carioca2\"]\n","files = sorted(sum([list((DATA_DIR / dset).glob(\"*.npy\")) for dset in dsets], list()))\n","# auth_files = [file for file in files if \"auth\" in file.stem]\n","# tamp_files = [file for file in files if \"tamp\" in file.stem]\n","# files = auth_files[::2] + tamp_files[1::2]\n","labels = [int(\"tamp\" in file.stem) for file in files]\n","data = np.stack([np.load(file) for file in files])\n","\n","# Clip edges\n","data = np.array([seq[40:-40] for seq in data])\n","\n","# # Pad sequences\n","# data_padded = [torch.tensor(seq, dtype=torch.float32) for seq in data]\n","# data_padded = pad_sequence(data_padded, batch_first=True, padding_value=NOMINAL_ENF)\n","\n","# Berechne den Median und den IQR jeder Zeitreihe\n","data_median = np.median(data, axis=1, keepdims=True)\n","data_iqr = np.percentile(data, 75, axis=1, keepdims=True) - np.percentile(data, 25, axis=1, keepdims=True)\n","\n","# Normalisiere die Zeitreihen\n","data_normalized = (data - data_median) / data_iqr\n","\n","# Konvertiere die normalisierten Daten zur√ºck zu einem Tensor\n","data_normalized = torch.tensor(data_normalized, dtype=torch.float32)\n","\n","input_size = (1, data_normalized.shape[1])\n","\n","# Split the data into training, validation, and test sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    data_normalized, labels, test_size=TEST_SIZE, random_state=0, stratify=labels\n",")\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train, y_train, test_size=TEST_SIZE, random_state=0, stratify=y_train\n",")\n","\n","# Convert data to tensors\n","X_train = torch.Tensor(X_train).unsqueeze(1).to(device)\n","y_train = torch.LongTensor(y_train).to(device)\n","X_val = torch.Tensor(X_val).unsqueeze(1).to(device)\n","y_val = torch.LongTensor(y_val).to(device)\n","X_test = torch.Tensor(X_test).unsqueeze(1).to(device)\n","y_test = torch.LongTensor(y_test).to(device)\n","\n","# # Check label distribution\n","# unique, counts = np.unique(y_train.cpu(), return_counts=True)\n","# class_distribution = dict(zip(unique, counts))\n","# print(f\"y_train distribution: {class_distribution}\")\n","# unique, counts = np.unique(y_val.cpu(), return_counts=True)\n","# class_distribution = dict(zip(unique, counts))\n","# print(f\"y_val distribution: {class_distribution}\")\n","# unique, counts = np.unique(y_test.cpu(), return_counts=True)\n","# class_distribution = dict(zip(unique, counts))\n","# print(f\"y_test distribution: {class_distribution}\")\n","\n","# Create TensorDatasets\n","X_train_dataset = TensorDataset(X_train, y_train)\n","X_val_dataset = TensorDataset(X_val, y_val)\n","X_test_dataset = TensorDataset(X_test, y_test)\n","\n","# Data loaders\n","X_train_loader = DataLoader(X_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","X_val_loader = DataLoader(X_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","X_test_loader = DataLoader(X_test_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:41:47.343118Z","iopub.status.busy":"2024-09-30T13:41:47.342745Z","iopub.status.idle":"2024-09-30T13:41:47.347994Z","shell.execute_reply":"2024-09-30T13:41:47.347079Z","shell.execute_reply.started":"2024-09-30T13:41:47.343078Z"},"trusted":true},"outputs":[],"source":["print(type(data))\n","print(data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:41:47.349969Z","iopub.status.busy":"2024-09-30T13:41:47.349495Z","iopub.status.idle":"2024-09-30T13:41:47.612716Z","shell.execute_reply":"2024-09-30T13:41:47.611738Z","shell.execute_reply.started":"2024-09-30T13:41:47.349925Z"},"trusted":true},"outputs":[],"source":["# Plot a sample from the data\n","plt.figure(figsize=(10, 2))\n","plt.plot(data_normalized[1])\n","plt.title(f\"Example Data Plot\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:41:47.615836Z","iopub.status.busy":"2024-09-30T13:41:47.615507Z","iopub.status.idle":"2024-09-30T13:41:47.651227Z","shell.execute_reply":"2024-09-30T13:41:47.65024Z","shell.execute_reply.started":"2024-09-30T13:41:47.615791Z"},"trusted":true},"outputs":[],"source":["# Print a single batch from the DataLoader\n","inputs, labels = next(iter(X_train_loader))\n","print(f\"Input batch shape: {inputs.shape}\")\n","print(f\"Labels batch: {labels}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:41:47.652618Z","iopub.status.busy":"2024-09-30T13:41:47.65231Z","iopub.status.idle":"2024-09-30T13:41:48.651639Z","shell.execute_reply":"2024-09-30T13:41:48.650863Z","shell.execute_reply.started":"2024-09-30T13:41:47.652585Z"},"trusted":true},"outputs":[],"source":["# Initialize loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n","# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:41:48.653255Z","iopub.status.busy":"2024-09-30T13:41:48.652754Z","iopub.status.idle":"2024-09-30T13:41:49.010005Z","shell.execute_reply":"2024-09-30T13:41:49.008968Z","shell.execute_reply.started":"2024-09-30T13:41:48.653218Z"},"trusted":true},"outputs":[],"source":["# Display model summary\n","summary(model, input_size=input_size)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:41:49.01153Z","iopub.status.busy":"2024-09-30T13:41:49.011213Z","iopub.status.idle":"2024-09-30T13:45:42.632783Z","shell.execute_reply":"2024-09-30T13:45:42.631927Z","shell.execute_reply.started":"2024-09-30T13:41:49.011496Z"},"trusted":true},"outputs":[],"source":["# Initialize EarlyStopping object\n","# early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n","\n","# Training and validation loop\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","\n","for epoch in (pbar:=tqdm(range(NUM_EPOCHS), desc=\"Training Progress\")):\n","    # Training\n","    model.train()\n","    train_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","    for inputs, labels in X_train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","\n","        _, predicted = torch.max(outputs, 1)\n","        total_train += labels.size(0)\n","        correct_train += (predicted == labels).sum().item()\n","\n","    train_loss /= len(X_train_loader)\n","    train_losses.append(train_loss)\n","    train_accuracy = correct_train / total_train\n","    train_accuracies.append(train_accuracy)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","    with torch.no_grad():\n","        for inputs, labels in X_val_loader:\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs, 1)\n","            total_val += labels.size(0)\n","            correct_val += (predicted == labels).sum().item()\n","\n","    val_loss /= len(X_val_loader)\n","    val_losses.append(val_loss)\n","    val_accuracy = correct_val / total_val\n","    val_accuracies.append(val_accuracy)\n","\n","    pbar.set_postfix_str(f\"Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n","    if (epoch + 1) % 100 == 0 or epoch == 0:\n","        tqdm.write(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n","\n","\"\"\"    # Check early stopping condition\n","    early_stopping(val_loss)\n","    if early_stopping.early_stop:\n","        print(\"Early stopping\")\n","        break\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:49:56.802234Z","iopub.status.busy":"2024-09-30T13:49:56.801303Z","iopub.status.idle":"2024-09-30T13:49:57.494314Z","shell.execute_reply":"2024-09-30T13:49:57.493386Z","shell.execute_reply.started":"2024-09-30T13:49:56.80218Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score, confusion_matrix\n","import seaborn as sns\n","\n","model.eval()\n","test_loss = 0.0\n","\n","predictions = []\n","labels = []\n","with torch.no_grad():\n","    for inputs, _labels in X_test_loader:\n","        outputs = model(inputs)\n","        loss = criterion(outputs, _labels)\n","        test_loss += loss.item()\n","        _, predicted = torch.max(outputs, 1)\n","        predictions.extend(predicted.cpu())\n","        labels.extend(_labels.cpu())\n","\n","cm = confusion_matrix(labels, predictions)\n","print(\"Confusion Matrix:\")\n","print(cm)\n","accuracy = accuracy_score(labels, predictions)\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","\n","# Plot the confusion matrix\n","plt.figure(figsize=(6, 4))\n","sns.heatmap(\n","    cm,\n","    annot=True,\n","    fmt=\".0f\",\n","    cmap=\"Blues\",\n","    xticklabels=[\"Authentic\", \"Tampered\"],\n","    yticklabels=[\"Authentic\", \"Tampered\"],\n",")\n","plt.xlabel(\"Predicted Class\")\n","plt.ylabel(\"True Class\")\n","plt.title(f\"Confusion Matrix of CNN All Data\")\n","plt.savefig(f\"cm_cnn.pdf\", dpi=300)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T13:48:27.569435Z","iopub.status.busy":"2024-09-30T13:48:27.569039Z","iopub.status.idle":"2024-09-30T13:48:28.19008Z","shell.execute_reply":"2024-09-30T13:48:28.189198Z","shell.execute_reply.started":"2024-09-30T13:48:27.569396Z"},"trusted":true},"outputs":[],"source":["# Plot the training and validation losses and accuracies\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_losses, label=\"Train Loss\", alpha=1)\n","plt.plot(val_losses, label=\"Validation Loss\", alpha=1)\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.title(\"Training and Validation Loss of CNN All Data\")\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(train_accuracies, label=\"Train Accuracy\", alpha=1)\n","plt.plot(val_accuracies, label=\"Validation Accuracy\", alpha=1)\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.title(\"Training and Validation Accuracy of CNN All Data\")\n","\n","plt.savefig(\"cnn_losses.pdf\", dpi=300)\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the model\n","torch.save(model.state_dict(), \"onedcnn_model_2_alldata.pth\")\n","print(\"Model saved successfully!\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5748547,"sourceId":9457400,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
